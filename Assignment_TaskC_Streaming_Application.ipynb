{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C_2: Streaming Application\n",
    "## Please check the terminal to see the running process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pprint import pprint\n",
    "import geohash\n",
    "#print('geohash for 88,22',geohash.encode(88,22,precision=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad6fccc04b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 10 minutes just in case no detection of producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;31m# ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def joinDataAndSendToDB(iter):\n",
    "    # create connection to Mongo and setup collection\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    coll_hotspot = db.hotspot\n",
    "    coll_climate = db.climate\n",
    "    empty_list = []\n",
    "    client = MongoClient()\n",
    "    print('----------------')\n",
    "    db = client.fit5148_assignment_db\n",
    "    \n",
    "    # set up 3 lists and append data from different producer accordingly\n",
    "    list_producer1 = []\n",
    "    list_producer2 = []\n",
    "    list_producer3 = []\n",
    "    list_hotspot_backup = []\n",
    "    for record in iter:\n",
    "        data = json.loads(record[1])\n",
    "        sender_id = data['sender_id']\n",
    "        print(sender_id)\n",
    "        if sender_id == 'producer_1':\n",
    "            print('climate')\n",
    "            print(data)\n",
    "            list_producer1.append(data)\n",
    "        elif sender_id == 'producer_2':\n",
    "            print('aqua')\n",
    "            print(data)\n",
    "            list_producer2.append(data)\n",
    "        else: \n",
    "            print('terrar')\n",
    "            print(data)\n",
    "            list_producer3.append(data)       \n",
    "    print('Start finding stuff to join')\n",
    "    \n",
    "    # if there are no record from both satellite, insert the climate data to Mongo directly\n",
    "    if len(list_producer2) == 0 and len(list_producer3) == 0:\n",
    "        print(\"No data from both satellite\")        \n",
    "        try:\n",
    "            coll_climate.insert(data)\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    # If there is record coming from only one satellite, store the climate data and hotspot data into each collections\n",
    "    elif len(list_producer2) == 0 and len(list_producer3) != 0:\n",
    "        print(\"Only TERRA data\") \n",
    "        for c in list_producer1:\n",
    "            climate = {\n",
    "                'date':c['created_time'],\n",
    "                'air_temperature_celcius':c['air_temperature_celcius'],\n",
    "                'relative_humandity':c['relative_humidity'],\n",
    "                'windspeed_knots':c['windspeed_knots'],\n",
    "                'max_wind_speed':c['max_wind_speed'],\n",
    "                'precipitation':c['precipitation '],\n",
    "                'latitude':c['latitude'],\n",
    "                'longitude':c['longitude']\n",
    "            }\n",
    "            coll_climate.insert_one(climate)\n",
    "        # compare the location using geohash, if the location matched then join the data by appending the created time from \n",
    "        for p in list_producer3:\n",
    "            position3 = geohash.encode(p['latitude'],p['longitude'],precision = 5)\n",
    "            for c in list_producer1:\n",
    "                position1 = geohash.encode(c['latitude'],c['longitude'],precision = 5)\n",
    "                if position1 == position3:\n",
    "                    hotspot = {\n",
    "                    'latitude':p['latitude'],\n",
    "                    'longitude':p['longitude'],\n",
    "                    'confidence':p['confidence'],\n",
    "                    'surface_temperature':p['surface_temperature'],\n",
    "                    'created_time':c['created_time']    \n",
    "                }\n",
    "                    # insert the data to Mongo\n",
    "                    coll_hotspot.insert_one(hotspot)                    \n",
    "    elif len(list_producer2) != 0 and len(list_producer3) == 0:\n",
    "        print(\"Only AQUA data\")\n",
    "        for c in list_producer1:\n",
    "                    climate = {\n",
    "                        'date':c['created_time'],\n",
    "                        'air_temperature_celcius':c['air_temperature_celcius'],\n",
    "                        'relative_humandity':c['relative_humidity'],\n",
    "                        'windspeed_knots':c['windspeed_knots'],\n",
    "                        'max_wind_speed':c['max_wind_speed'],\n",
    "                        'precipitation':c['precipitation '],\n",
    "                        'latitude':c['latitude'],\n",
    "                        'longitude':c['longitude']\n",
    "                    }\n",
    "                    coll_climate.insert_one(climate)\n",
    "            \n",
    "        for p in list_producer2:\n",
    "            position3 = geohash.encode(p['latitude'],p['longitude'],precision = 5)\n",
    "            for c in list_producer1:\n",
    "                position1 = geohash.encode(c['latitude'],c['longitude'],precision = 5)\n",
    "                if position1 == position3:\n",
    "                    hotspot = {\n",
    "                    'latitude':p['latitude'],\n",
    "                    'longitude':p['longitude'],\n",
    "                    'confidence':p['confidence'],\n",
    "                    'surface_temperature':p['surface_temperature'],\n",
    "                    'created_time':c['created_time']    \n",
    "                }\n",
    "                    # insert the data to Mongo\n",
    "                    coll_hotspot.insert_one(hotspot)\n",
    "    # if there is data coming from both satellite, then compare the data and average the confidence and surface_temperature if the location matched                    \n",
    "    elif len(list_producer2) != 0 and len(list_producer3) != 0:\n",
    "        print(\"Receive data from both satellites\")\n",
    "        for c in list_producer1:\n",
    "                    climate = {\n",
    "                        'date':c['created_time'],\n",
    "                        'air_temperature_celcius':c['air_temperature_celcius'],\n",
    "                        'relative_humandity':c['relative_humidity'],\n",
    "                        'windspeed_knots':c['windspeed_knots'],\n",
    "                        'max_wind_speed':c['max_wind_speed'],\n",
    "                        'precipitation':c['precipitation '],\n",
    "                        'latitude':c['latitude'],\n",
    "                        'longitude':c['longitude']\n",
    "                    }\n",
    "                    coll_climate.insert_one(climate)\n",
    "                    \n",
    "        for i in list_producer2:\n",
    "            position2 = geohash.encode(i['latitude'],i['longitude'],precision = 5)\n",
    "            coll_hotspot\n",
    "            for j in list_producer3:\n",
    "                position3 = geohash.encode(j['latitude'],j['longitude'],precision = 5)\n",
    "                if position2 == position3:\n",
    "                    hotspot = {\n",
    "                        'latitude':i['latitude'],\n",
    "                        'longitude':i['longitude'],\n",
    "                        'confidence':(i['confidence']+j['confidence'])/2,\n",
    "                        'surface_temperature':(i['surface_temperature']+j['surface_temperature'])/2\n",
    "                    } \n",
    "                    # Append all data to this list after average\n",
    "                    list_hotspot_backup.append(hotspot)\n",
    "        \n",
    "        for h in list_hotspot_backup:\n",
    "            position3 = geohash.encode(h['latitude'],h['longitude'],precision = 5)\n",
    "            for c in list_producer1:\n",
    "                position1 = geohash.encode(c['latitude'],c['longitude'],precision = 5)\n",
    "                if position1 == position3:\n",
    "                    hotspot = {\n",
    "                    'latitude':h['latitude'],\n",
    "                    'longitude':h['longitude'],\n",
    "                    'confidence':h['confidence'],\n",
    "                    'surface_temperature':h['surface_temperature'],\n",
    "                    'created_time':h['created_time']    \n",
    "                }\n",
    "                    # insert joined hotspot into Mongo\n",
    "                    coll_hotspot.insert_one(hotspot)\n",
    "                          \n",
    "    client.close()\n",
    "\n",
    "# set the spark interval to 10 seconds\n",
    "n_secs = 10\n",
    "\n",
    "# set up topic to subscribe\n",
    "topic = \"StopFire\"\n",
    "\n",
    "# create connection\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "data = []\n",
    "#print('success')\n",
    "# perform join operation and send data to Mongo\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(joinDataAndSendToDB))\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
